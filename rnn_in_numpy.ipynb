{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "RNN training using numpy code.  Based on the [works](https://gist.github.com/karpathy/d4dee566867f8291f086) of A Karpathy.  Modified to use RELU instead of logistic function."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "# read full file\n",
      "data = open( 'pg_essays.txt', 'rt' ).read()\n",
      "\n",
      "# dictionary conversion. We will use n-gram coding for all the unique characters in the input data\n",
      "dict_chars = list( set( data ) )\n",
      "data_size = len( data )\n",
      "dict_size = len( dict_chars )\n",
      "\n",
      "# character encoding\n",
      "char_to_x = { c:i for i, c in enumerate( dict_chars ) }\n",
      "x_to_char = { i:c for i, c in enumerate( dict_chars ) }\n",
      "\n",
      "print( 'data size: {0}, dictionary size: {1}'.format( data_size, dict_size ) )\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "data size: 1538444, dictionary size: 120\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# net structure\n",
      "hidden_nodes = 100\n",
      "seq_len = 25\n",
      "\n",
      "learning_rate = 1e-1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "RNN equations with normalisation\n",
      "\n",
      "$$ h' = f( W_{hh} h + W_{xh} x + b_h ) $$\n",
      "$$ y = W_{hy} h' + b_y $$\n",
      "$$ p = \\left[ \\frac{e^{y_i}}{\\sum{ e^{y_i} } }, ... \\right]^T $$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# model weights - drawn initially from guassian distribution\n",
      "Whh = np.random.normal( 0., 2., (hidden_nodes, hidden_nodes) )\n",
      "Wxh = np.random.normal( 0., 2., (hidden_nodes, dict_size ) )\n",
      "Wyh = np.random.normal( 0., 2., (dict_size, hidden_nodes ) )\n",
      "\n",
      "bh = np.zeros( (hidden_nodes, 1) )\n",
      "by = np.zeros( (dict_size, 1 ) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# calculate loss, model weights gradients and return hidden state for propagation\n",
      "def loss( inputs, targets, hprev ):\n",
      "    '''\n",
      "    input and target have len of seq_len\n",
      "    '''\n",
      "    \n",
      "    xs, hraw, hs, ys, ps = {}, {}, {}, {}, {}\n",
      "    hs[ -1 ] = hprev\n",
      "    loss = 0.\n",
      "    \n",
      "    # forward pass\n",
      "    for i in range( len( inputs ) ):\n",
      "        \n",
      "        # encode input character\n",
      "        xs[i] = np.zeros( ( dict_size, 1 ) )\n",
      "        xs[i][inputs[i]] = 1.\n",
      "        \n",
      "        hraw[i] = np.dot( Wxh, xs[i] ) + np.dot( Whh, hs[i-1] ) + bh\n",
      "        hs[i] = np.maximum( hraw[i], 0. )\n",
      "        ys[i] = np.dot( Wyh, hs[i] )\n",
      "        \n",
      "        # normalise probabilities\n",
      "        ps[i] = np.exp( ys[i] ) / np.sum( np.exp( ys[i] ) )\n",
      "        \n",
      "        # softmax (cross-entropy loss)\n",
      "        loss += -np.log( ps[i][targets[i],0] )\n",
      "\n",
      "    dWxh, dWhh, dWhy = np.zeros_like( Wxh ), np.zeros_like( Whh ), np.zeros_like( Why )\n",
      "    dbh, dby = np.zeros_like( bh ), np.zeros_like( by )\n",
      "    dhnext = np.zeros_like( hs[0] )\n",
      "    \n",
      "    # backward pass: start from the end\n",
      "    for i in reverse( range( len( inputs ) ) ):\n",
      "        dy = np.copy( ps[i] )\n",
      "        dy[targets[i]] -= 1.0    # backprop into y.  In the target state the ps[ target[i] ] == 1.0\n",
      "\n",
      "        # recover weights\n",
      "        dWhy += np.dot( dy, hs[i].T )\n",
      "        dby += dy\n",
      "        \n",
      "        dh = np.dot( Why.T, dy) + dhnext # backprop into h\n",
      "        dhtemp = np.zeros_like( dhnext )\n",
      "        dhtemp[ hraw[i] > 0. ] = 1.\n",
      "        dhraw = dhtemp * dh\n",
      "        \n",
      "        dbh += dhraw\n",
      "        dWxh += np.dot( dhraw, xs[t].T )\n",
      "        dWhh += np.dot( dhraw, hs[t-1].T )\n",
      "        dhnext = np.dot( Whh.T, dhraw )\n",
      "        \n",
      "        # clip to mitigate exploding gradients\n",
      "        for dparam in [ dWxh, dWhh, dWhy, dbh, dby ]:\n",
      "            np.clip( dparam, -5, 5, out=dparam )\n",
      "            \n",
      "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[ len(inputs)-1 ]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# gradient validation by A Karpathy\n",
      "from random import uniform\n",
      "\n",
      "def gradCheck(inputs, targets, hprev):\n",
      "    global Wxh, Whh, Why, bh, by\n",
      "\n",
      "    num_checks, delta = 10, 1e-5\n",
      "\n",
      "    # calculate gradients using backprop\n",
      "    _, dWxh, dWhh, dWhy, dbh, dby, _ = loss( inputs, targets, hprev )\n",
      "\n",
      "    for param, dparam, name in zip([Wxh, Whh, Why, bh, by], [dWxh, dWhh, dWhy, dbh, dby], ['Wxh', 'Whh', 'Why', 'bh', 'by']):\n",
      "        s0 = dparam.shape\n",
      "        s1 = param.shape\n",
      "        assert s0 == s1, 'Error dims dont match: %s and %s.' % (`s0`, `s1`)\n",
      "        print( name )\n",
      "\n",
      "        for i in xrange(num_checks):\n",
      "            ri = int( uniform(0,param.size) )\n",
      "            \n",
      "            # evaluate cost at [x + delta] and [x - delta]\n",
      "            old_val = param.flat[ri]\n",
      "            param.flat[ri] = old_val + delta\n",
      "            cg0, _, _, _, _, _, _ = lossFun( inputs, targets, hprev )\n",
      "            param.flat[ri] = old_val - delta\n",
      "            cg1, _, _, _, _, _, _ = lossFun( inputs, targets, hprev )\n",
      "            param.flat[ri] = old_val # reset old value for this parameter\n",
      " \n",
      "            # fetch both numerical and analytic gradient\n",
      "            grad_analytic = dparam.flat[ri]\n",
      "            grad_numerical = (cg0 - cg1) / ( 2 * delta )\n",
      "            rel_error = abs( grad_analytic - grad_numerical ) / abs( grad_numerical + grad_analytic )\n",
      "\n",
      "            print( '%f, %f => %e ' % ( grad_numerical, grad_analytic, rel_error) )\n",
      "            # rel_error should be on order of 1e-7 or less\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "p = 0\n",
      "inputs = [ char_to_x[ c ] for c in data[ p:p+seq_len ] ]\n",
      "targets = [ char_to_x[ c ] for c in data[ p + 1:p+seq_len + 1 ] ]\n",
      "\n",
      "hprev = np.zeros_like( bh )\n",
      "\n",
      "print( data[ p:p+seq_len ], inputs )\n",
      "print( data[ p + 1:p+seq_len + 1 ], targets )\n",
      "\n",
      "# run gradianet check\n",
      "gradCheck( inputs, target, hprev )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ValueError",
       "evalue": "invalid literal for long() with base 10: 'r'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-24-fcefbc1045a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# run gradianet check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mgradCheck\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhprev\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m<ipython-input-23-80920421cc0a>\u001b[0m in \u001b[0;36mgradCheck\u001b[0;34m(inputs, targets, hprev)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# calculate gradients using backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWxh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhprev\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mWxh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWhh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdWxh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdby\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Wxh'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Whh'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Why'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bh'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'by'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-17-451a4f3f97d5>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(inputs, targets, hprev)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# softmax (cross-entropy loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mdWxh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mWxh\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mWhh\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mWhy\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mValueError\u001b[0m: invalid literal for long() with base 10: 'r'"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('Programming Bottom-Up\\r\\n\\r\\n', [13, 85, 53, 50, 85, 108, 113, 113, 111, 83, 50, 1, 67, 53, 26, 26, 53, 113, 93, 105, 24, 89, 59, 89, 59])\n",
        "('rogramming Bottom-Up\\r\\n\\r\\n(', [85, 53, 50, 85, 108, 113, 113, 111, 83, 50, 1, 67, 53, 26, 26, 53, 113, 93, 105, 24, 89, 59, 89, 59, 3])\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    }
   ],
   "metadata": {}
  }
 ]
}